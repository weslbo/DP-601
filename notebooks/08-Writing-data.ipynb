{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "9ce64dd7-18a1-4125-a99f-c410abdb9036",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "# Writing Data\n",
                "\n",
                "Just as there are many ways to read data, we have just as many ways to write data.\n",
                "\n",
                "In this notebook, we will take a quick peek at how to write data back out to Parquet files.\n",
                "\n",
                "**Technical Accomplishments:**\n",
                "- Writing data to Parquet files"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0ae61f51-d065-41e5-b934-c1671d555444",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [],
            "source": [
                "%run Utilities"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ef63b739-b608-47f3-b69e-6d8886388b47",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "## ➡️ Writing Data\n",
                "\n",
                "Let's start with one of our original CSV data sources, **pageviews_by_second.tsv**:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b2a59664-3ceb-4254-a528-6347901909f6",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [],
            "source": [
                "from pyspark.sql.types import *\n",
                "\n",
                "csvFile = \"Files/sampledata/pageviews-by-second.tsv\"\n",
                "\n",
                "csvSchema = StructType([\n",
                "  StructField(\"timestamp\", StringType(), False),\n",
                "  StructField(\"site\", StringType(), False),\n",
                "  StructField(\"requests\", IntegerType(), False)\n",
                "])\n",
                "\n",
                "csvDF = (spark.read                        # The DataFrameReader\n",
                "   .option(\"header\", \"true\")       # Use first line of all files as header\n",
                "   .option(\"sep\", \"\\t\")            # Use tab delimiter (default is comma-separator)\n",
                "   .option(\"inferSchema\", \"true\")  # Automatically infer data types\n",
                "   .csv(csvFile)                   # Creates a DataFrame from CSV after reading in the file\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "79b9d079-3ad1-4018-8abf-d5da231562ae",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "Now that we have a `DataFrame`, we can write it back out as Parquet files or other various formats."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a51da335-46ad-4769-ba75-91da272b0b33",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [],
            "source": [
                "fileName = \"Files/sampledata/pageviews-by-second.parquet\"\n",
                "print(\"Output location: \" + fileName)\n",
                "\n",
                "(csvDF.write                       # Our DataFrameWriter\n",
                "  .option(\"compression\", \"snappy\") # One of none, snappy, gzip, and lzo\n",
                "  .mode(\"overwrite\")               # Replace existing files\n",
                "  .parquet(fileName)               # Write DataFrame to Parquet files\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c604855e-83b7-47da-9ccc-242acebeab60",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "Now that the file has been written out, we can see it in the lakehouse:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "791d0dda-0b4a-4f88-8b29-939496963fd1",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [],
            "source": [
                "%%sh\n",
                "ls /lakehouse/default/Files/sampledata"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c03958c7-db15-484a-8642-6e154b2e122c",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "And lastly we can read that same parquet file back in and display the results:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b04e97de-4ed4-44c4-9db2-14478a9f96d6",
            "metadata": {
                "collapsed": false,
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [],
            "source": [
                "display(\n",
                "  spark.read.parquet(fileName)\n",
                ")"
            ]
        }
    ],
    "metadata": {
        "kernel_info": {
            "name": "synapse_pyspark"
        },
        "kernelspec": {
            "display_name": "Synapse PySpark",
            "name": "synapse_pyspark"
        },
        "language_info": {
            "name": "python"
        },
        "microsoft": {
            "host": { },
            "language": "python",
            "ms_spell_check": {
                "ms_spell_check_language": "en"
            }
        },
        "notebook_environment": {},
        "nteract": {
            "version": "nteract-front-end@1.0.0"
        },
        "save_output": true,
        "spark_compute": {
            "compute_id": "/trident/default",
            "session_options": {
                "conf": {},
                "enableDebugMode": false
            }
        },
        "synapse_widget": {
            "state": {},
            "version": "0.1"
        },
        "trident": {
            "lakehouse": {}
        },
        "widgets": {}
    },
    "nbformat": 4,
    "nbformat_minor": 5
}