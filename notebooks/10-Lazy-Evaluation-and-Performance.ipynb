{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "b251f5e9-ef5e-46a0-8c8b-80905504c13e",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "# Describe the difference between eager and lazy execution\n",
                "\n",
                "## ➡️Getting Started\n",
                "\n",
                "Run the following cell to configure our notebook."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1b5f967e-88aa-4b47-9197-82013a8060f7",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [],
            "source": [
                "%run Utilities"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0b4abf81-4b6d-44b8-ad9e-3ba8ab0ba82e",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "## ➡️ Laziness By Design\n",
                "\n",
                "Fundamental to Apache Spark are the notions that\n",
                "* Transformations are **LAZY**\n",
                "* Actions are **EAGER**\n",
                "\n",
                "The following code condenses the logic from the DataFrames modules in this learning path, and uses the DataFrames API to:\n",
                "- Specify a schema, format, and file source for the data to be loaded\n",
                "- Select columns to `GROUP BY`\n",
                "- Aggregate with a `COUNT`\n",
                "- Provide an alias name for the aggregate output\n",
                "- Specify a column to sort on\n",
                "\n",
                "This cell defines a series of **transformations**. By definition, this logic will result in a DataFrame and will not trigger any jobs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f07b38cf-673e-4306-a451-d5f8caa1a8f4",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [],
            "source": [
                "parquetDir = \"Files/taxidata/yellow*.parquet\"\n",
                "\n",
                "countsDF = (spark         # Our SparkSession & Entry Point\n",
                "  .read                     # Our DataFrameReader\n",
                "  .parquet(parquetDir)      # Returns an instance of DataFrame\n",
                "  .groupBy(\"VendorID\")\n",
                "  .count()\n",
                "  .withColumnRenamed(\"count\", \"Counts\")\n",
                "  .orderBy(\"VendorID\")\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "dbf04147-5108-44f1-a779-c39c0ad92c51",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "Because `display` is an **action**, a job _will_ be triggered, as logic is executed against the specified data to return a result."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "db9091a1-48e5-4c0f-bc0f-89f309f022a4",
            "metadata": {
                "collapsed": false,
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [],
            "source": [
                "display(countsDF)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e3ac2591-e7a3-4ebe-a635-235ea34164bb",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### ➡️ Why is Laziness So Important?\n",
                "\n",
                "Laziness is at the core of Scala and Spark.\n",
                "\n",
                "It has a number of benefits:\n",
                "* Not forced to load all data at step #1\n",
                "  * Technically impossible with **REALLY** large datasets.\n",
                "* Easier to parallelize operations\n",
                "  * N different transformations can be processed on a single data element, on a single thread, on a single machine.\n",
                "* Optimizations can be applied prior to code compilation"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b8b33601-3744-4bd9-b4ec-22c8ceabf4cc",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "# Actions & Transformations\n",
                "\n",
                "## ➡️ Actions\n",
                "\n",
                "In production code, actions will generally **write data to persistent storage** using the DataFrameWriter.\n",
                "\n",
                "During interactive code development in Fabric notebooks, the `display` method will frequently be used to **materialize a view of the data** after logic has been applied.\n",
                "\n",
                "A number of other actions provide the ability to return previews or specify physical execution plans for how logic will map to data. For the complete list, review the [API docs](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset).\n",
                "\n",
                "| Method | Return | Description |\n",
                "|--------|--------|-------------|\n",
                "| `collect()` | Collection | Returns an array that contains all of Rows in this Dataset. |\n",
                "| `count()` | Long | Returns the number of rows in the Dataset. |\n",
                "| `first()` | Row | Returns the first row. |\n",
                "| `foreach(f)` | - | Applies a function f to all rows. |\n",
                "| `foreachPartition(f)` | - | Applies a function f to each partition of this Dataset. |\n",
                "| `head()` | Row | Returns the first row. |\n",
                "| `reduce(f)` | Row | Reduces the elements of this Dataset using the specified binary function. |\n",
                "| `show(..)` | - | Displays the top 20 rows of Dataset in a tabular form. |\n",
                "| `take(n)` | Collection | Returns the first n rows in the Dataset. |\n",
                "| `toLocalIterator()` | Iterator | Return an iterator that contains all of Rows in this Dataset. |\n",
                "\n",
                "❗ Actions such as `collect` can lead to out of memory errors by forcing the collection of all data."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "274426de-702d-4fe0-93ec-c4e2467201cc",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "## ➡️ Transformations\n",
                "\n",
                "Transformations have the following key characteristics:\n",
                "* They eventually return another `DataFrame`.\n",
                "* They are immutable - that is each instance of a `DataFrame` cannot be altered once it's instantiated.\n",
                "  * This means other optimizations are possible - such as the use of shuffle files (to be discussed in detail later)\n",
                "* Are classified as either a Wide or Narrow operation\n",
                "\n",
                "Most operations in Spark are **transformations**. While many transformations are [DataFrame operations](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset), writing efficient Spark code will require importing methods from the `sql.functions` module, which contains [transformations corresponding to SQL built-in operations](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$)."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a181ca14-fbf7-4b12-9306-530d15afd616",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "## ➡️ Types of Transformations\n",
                "\n",
                "A transformation may be wide or narrow.\n",
                "\n",
                "A wide transformation requires sharing data across workers. \n",
                "\n",
                "A narrow transformation can be applied per partition/worker with no need to share or shuffle data to other workers. "
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9f7d82d2-a4a3-44be-86ef-1b4444fdb99a",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "## ➡️ Narrow Transformations\n",
                "\n",
                "The data required to compute the records in a single partition reside in at most one partition of the parent Dataframe.\n",
                "\n",
                "Examples include:\n",
                "* `filter(..)`\n",
                "* `drop(..)`\n",
                "* `coalesce()`\n",
                "\n",
                "![](https://github.com/weslbo/DP-601/blob/main/images/Narrow-Transformation.png?raw=true)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "36bf7f03-6ce7-4293-bf5f-fd9026076fe6",
            "metadata": {
                "collapsed": false,
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [],
            "source": [
                "from pyspark.sql.functions import col\n",
                "\n",
                "pickupLocation249 = (spark.read\n",
                "                          .parquet(parquetDir)\n",
                "                          .filter(col(\"PULocationID\") == 249)\n",
                "                          .limit(10))\n",
                "\n",
                "display(pickupLocation249)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0197cc24-4c62-48d7-b385-9784c9164e76",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "## ➡️ Wide Transformations\n",
                "\n",
                "The data required to compute the records in a single partition may reside in many partitions of the parent Dataframe. These operations require that data is **shuffled** between executors.\n",
                "\n",
                "Examples include:\n",
                "* `distinct()`\n",
                "* `groupBy(..).sum()`\n",
                "* `repartition(n)`\n",
                "\n",
                "![](https://github.com/weslbo/DP-601/blob/main/images/Wide-Transformation.png?raw=true)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "08fd1fea-77b4-4d34-a207-f5f131010af4",
            "metadata": {
                "collapsed": false,
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [],
            "source": [
                "from pyspark.sql.functions import col, desc\n",
                "\n",
                "pickupLocations = (spark.read\n",
                "                          .parquet(parquetDir)\n",
                "                          .groupBy(\"PULocationID\")\n",
                "                          .sum(\"passenger_count\")\n",
                "                          .withColumnRenamed(\"sum(passenger_count)\", \"sum_passenger_count\")\n",
                "                          .sort(desc(\"sum_passenger_count\"))\n",
                "                          .limit(10))\n",
                "\n",
                "display(pickupLocations)"
            ]
        }
    ],
    "metadata": {
        "kernel_info": {
            "name": "synapse_pyspark"
        },
        "kernelspec": {
            "display_name": "Synapse PySpark",
            "language": "Python",
            "name": "synapse_pyspark"
        },
        "language_info": {
            "name": "python"
        },
        "microsoft": {
            "host": {},
            "language": "python",
            "ms_spell_check": {
                "ms_spell_check_language": "en"
            }
        },
        "notebook_environment": {},
        "nteract": {
            "version": "nteract-front-end@1.0.0"
        },
        "save_output": true,
        "spark_compute": {
            "compute_id": "/trident/default",
            "session_options": {
                "conf": {},
                "enableDebugMode": false
            }
        },
        "synapse_widget": {
            "state": {},
            "version": "0.1"
        },
        "trident": {
            "lakehouse": {}
        },
        "widgets": {}
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
