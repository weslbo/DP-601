{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Reading Data - JSON Files\n",
                "\n",
                "**Technical Accomplishments:**\n",
                "- Read data from:\n",
                "  * JSON without a Schema\n",
                "  * JSON with a Schema"
            ],
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "id": "6b5687f6-9db2-421d-9642-ab04ef4be8f9"
        },
        {
            "cell_type": "markdown",
            "source": [
                "## ➡️ Getting Started\n",
                "\n",
                "Run the following cell to configure our notebook.\""
            ],
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "id": "0170ab7b-6a6f-4603-ac07-714c63a67e0f"
        },
        {
            "cell_type": "code",
            "source": [
                "%run Utilities"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "id": "c00c3de4-e1f9-4bba-8373-041fbc8ad5a9"
        },
        {
            "cell_type": "markdown",
            "source": [
                "## ➡️ Reading from JSON w/ InferSchema\n",
                "\n",
                "Reading in JSON isn't that much different than reading in CSV files.\n",
                "\n",
                "Let's start with taking a look at all the different options that go along with reading in JSON files."
            ],
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "id": "5169b698-7def-41a0-8d91-ac34fc513ebf"
        },
        {
            "cell_type": "markdown",
            "source": [
                "## ➡️ JSON Lines\n",
                "\n",
                "Much like the CSV reader, the JSON reader also assumes...\n",
                "* That there is one JSON object per line and...\n",
                "* That it's delineated by a new-line.\n",
                "\n",
                "This format is referred to as **JSON Lines** or **newline-delimited JSON** \n",
                "\n",
                "More information about this format can be found at <a href=\"http://jsonlines.org/\" target=\"_blank\">http://jsonlines.org</a>."
            ],
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "id": "bd3d3c5d-d5e3-45c9-a761-caf900303c46"
        },
        {
            "cell_type": "markdown",
            "source": [
                "## ➡️ The Data Source\n",
                "* For this exercise, we will be using a JSON file, containing some ZIP codes\n",
                "* Like we did with the CSV file, we can use **&percnt;&percnt;sh ls ...** to view the file on the DBFS."
            ],
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "id": "b3ee914d-d907-404d-b344-2f6a5f75c44c"
        },
        {
            "cell_type": "code",
            "source": [
                "%%sh\n",
                "\n",
                "ls /lakehouse/default/Files/sampledata/zipcodes_singlelines.json"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "id": "abef062c-79b8-4b00-a814-c5b4e9abfe44"
        },
        {
            "cell_type": "markdown",
            "source": [
                "Like we did with the CSV file, we can use %%sh head ... to peek at the first couple lines of the JSON file."
            ],
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "id": "0af8e9db-c6d7-46f0-b537-e63493cd98ea"
        },
        {
            "cell_type": "code",
            "source": [
                "%%sh\n",
                "\n",
                "head /lakehouse/default/Files/sampledata/zipcodes_singlelines.json"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "id": "8cb2492c-7282-4aff-b7d5-0a17cfba2a38"
        },
        {
            "cell_type": "markdown",
            "source": [
                "## ➡️ Read The JSON File\n",
                "\n",
                "The command to read in JSON looks very similar to that of CSV.\n",
                "\n",
                "In addition to reading the JSON file, we will also print the resulting schema."
            ],
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "id": "a7c7592d-9604-4d2f-96fc-27fb9fd8f4d5"
        },
        {
            "cell_type": "code",
            "source": [
                "jsonFile = \"Files/sampledata/zipcodes_singlelines.json\"\n",
                "\n",
                "zipcodesDF = (spark.read           # The DataFrameReader\n",
                "    .option(\"inferSchema\", \"true\")  # Automatically infer data types & column names\n",
                "    .json(jsonFile)                 # Creates a DataFrame from JSON after reading in the file\n",
                " )\n",
                "zipcodesDF.printSchema()"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "id": "6dd75e6f-37bd-40ed-8b3b-3a8e8118f451"
        },
        {
            "cell_type": "markdown",
            "source": [
                "With our DataFrame created, we can now take a peak at the data.\n",
                "\n",
                "But to demonstrate a unique aspect of JSON data (or any data with embedded fields), we will first create a temporary view and then view the data via SQL:"
            ],
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "id": "ea5480e4-d62e-4d98-8da9-73c37330deb1"
        },
        {
            "cell_type": "code",
            "source": [
                "# create a view called wiki_edits\n",
                "zipcodesDF.createOrReplaceTempView(\"zipcodes\")"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "id": "8909191c-5439-44dc-b827-30e147f21807"
        },
        {
            "cell_type": "markdown",
            "source": [
                "And now we can take a peak at the data with simple SQL SELECT statement:"
            ],
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "id": "8acf3c68-e89b-4bd0-bd34-31177dd33935"
        },
        {
            "cell_type": "code",
            "source": [
                "%%sql\n",
                "\n",
                "SELECT * FROM zipcodes"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                },
                "microsoft": {
                    "language": "sparksql"
                },
                "collapsed": false
            },
            "id": "ac5d3930-c4b9-4abc-8ca5-d0b6194e16ea"
        },
        {
            "cell_type": "markdown",
            "source": [
                "Notice the **geocoding** column has embedded data.\n",
                "\n",
                "You can expand the fields by clicking the right triangle in each row.\n",
                "\n",
                "But we can also reference the sub-fields directly as we see in the following SQL statement:"
            ],
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "id": "14679b06-1ad8-4e48-8c72-c56ccf5ded7c"
        },
        {
            "cell_type": "code",
            "source": [
                "%%sql\n",
                "\n",
                "SELECT City, Location, geocoding.Lat, geocoding.Long, geocoding.Zaxis\n",
                "FROM zipcodes\n",
                "WHERE geocoding.Zaxis > 0.5"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                },
                "microsoft": {
                    "language": "sparksql"
                },
                "collapsed": false
            },
            "id": "a37c6433-0ed3-4f80-8cf3-8fdd705a750f"
        },
        {
            "cell_type": "markdown",
            "source": [
                "## ➡️ Reading from JSON w/ User-Defined Schema\n",
                "\n",
                "To avoid the extra job, we can (just like we did with CSV) specify the schema for the `DataFrame`."
            ],
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "id": "83bb80bb-8dc5-4939-89fe-5de7f9e25686"
        },
        {
            "cell_type": "markdown",
            "source": [
                "## ➡️ Step #1 - Create the Schema\n",
                "\n",
                "Compared to our CSV example, the structure of this data is a little more complex.\n",
                "\n",
                "Note that we can support complex data types as seen in the field `geocoding`."
            ],
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "id": "db5c87a3-a34d-4168-a519-347e6c096efb"
        },
        {
            "cell_type": "code",
            "source": [
                "from pyspark.sql.types import *\n",
                "\n",
                "jsonSchema = StructType([\n",
                "    StructField(\"City\", StringType(), True),\n",
                "    StructField(\"Country\", StringType(), True),\n",
                "    StructField(\"Decommisioned\", BooleanType(), True),\n",
                "    StructField(\"EstimatedPopulation\", LongType(), True),\n",
                "    StructField(\"Location\", StringType(), True),\n",
                "    StructField(\"LocationText\", StringType(), True),\n",
                "    StructField(\"LocationType\", StringType(), True),\n",
                "    StructField(\"Notes\", StringType(), True),\n",
                "    StructField(\"RecordNumber\", LongType(), True),\n",
                "    StructField(\"State\", StringType(), True),\n",
                "    StructField(\"TaxReturnsFiled\", LongType(), True),\n",
                "    StructField(\"TotalWages\", LongType(), True),\n",
                "    StructField(\"WorldRegion\", StringType(), True),\n",
                "    StructField(\"ZipCodeType\", StringType(), True),\n",
                "    StructField(\"Zipcode\", LongType(), True),\n",
                "    StructField(\"geocoding\", StructType([\n",
                "        StructField(\"Lat\", DoubleType(), True),\n",
                "        StructField(\"Long\", DoubleType(), True),\n",
                "        StructField(\"Xaxis\", DoubleType(), True),\n",
                "        StructField(\"Yaxis\", DoubleType(), True),\n",
                "        StructField(\"Zaxis\", DoubleType(), True)\n",
                "    ]))\n",
                "])"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "id": "94f116c8-26ea-459f-983e-e96122657664"
        },
        {
            "cell_type": "markdown",
            "source": [
                "That was a lot of typing to get our schema!\n",
                "\n",
                "For a small file, manually creating the the schema may not be worth the effort.\n",
                "\n",
                "However, for a large file, the time to manually create the schema may be worth the trade off of a really long infer-schema process.\n",
                "\n",
                "## ➡️ Step #2 - Read in the JSON\n",
                "\n",
                "Next, we will read in the JSON file and once again print its schema."
            ],
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "id": "3410d3b2-a0d1-4f48-b6e1-e51fcfddce1a"
        },
        {
            "cell_type": "code",
            "source": [
                "(spark.read            # The DataFrameReader\n",
                "  .schema(jsonSchema)  # Use the specified schema\n",
                "  .json(jsonFile)      # Creates a DataFrame from JSON after reading in the file\n",
                "  .printSchema()\n",
                ")"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "id": "5a87a383-f77e-4ee4-ad45-9c1c60cc0f79"
        },
        {
            "cell_type": "markdown",
            "source": [
                "## ➡️ Review: Reading from JSON w/ User-Defined Schema\n",
                "* Just like CSV, providing the schema avoids the extra jobs.\n",
                "* The schema allows us to rename columns and specify alternate data types.\n",
                "* Can get arbitrarily complex in its structure.\n",
                "\n",
                "Let's take a look at some of the other details of the `DataFrame` we just created for comparison sake."
            ],
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "id": "57f887b8-649a-4288-8dcb-36266fe98a03"
        },
        {
            "cell_type": "code",
            "source": [
                "jsonDF = (spark.read\n",
                "  .schema(jsonSchema)\n",
                "  .json(jsonFile)    \n",
                ")\n",
                "\n",
                "print(\"Partitions: \" + str(jsonDF.rdd.getNumPartitions()))\n",
                "\n",
                "printRecordsPerPartition(jsonDF)\n",
                "\n",
                "print(\"-\"*80)"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "id": "74d10875-3ee9-4976-87c4-823fe1d2b2e3"
        },
        {
            "cell_type": "markdown",
            "source": [
                "And of course we can view that data here:"
            ],
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "id": "6a83d49c-c820-4929-88c1-21a6a6f93eb1"
        },
        {
            "cell_type": "code",
            "source": [
                "display(jsonDF)"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                },
                "collapsed": false
            },
            "id": "461bfff2-485f-43ff-9349-219a3b4bf4e4"
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        },
        "kernelspec": {
            "name": "synapse_pyspark",
            "display_name": "Synapse PySpark"
        },
        "microsoft": {
            "host": {},
            "language": "python",
            "ms_spell_check": {
                "ms_spell_check_language": "en"
            }
        },
        "widgets": {},
        "kernel_info": {
            "name": "synapse_pyspark"
        },
        "nteract": {
            "version": "nteract-front-end@1.0.0"
        },
        "save_output": true,
        "spark_compute": {
            "compute_id": "/trident/default",
            "session_options": {
                "enableDebugMode": false,
                "conf": {}
            }
        },
        "notebook_environment": {},
        "synapse_widget": {
            "version": "0.1",
            "state": {}
        },
        "trident": {
            "lakehouse": {}
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}